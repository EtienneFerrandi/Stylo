library(stylo)
setwd("~/Documents/two_subcorpora")
rolling.classify(write.png.file = TRUE, classification.method = "svm", distance.measure="delta", features="wordlist_stopwords.txt",
mfw = 1000, training.set.sampling = "normal.sampling", slice.size = 40, slice.overlap = 30, culling=0,
delete.pronouns=FALSE, preserve.case=FALSE, corpus.lang="Latin")
rolling.classify(write.png.file = TRUE, classification.method = "svm", distance.measure="delta", features="stopwords.txt",
mfw = 63, training.set.sampling = "normal.sampling", slice.size = 40, slice.overlap = 30, culling=0,
delete.pronouns=FALSE, preserve.case=FALSE, corpus.lang="Latin")
#test sur les textes non-lemmatisés et sur les trigrammes de caractères
setwd("~/Documents/two_subcorpora1")
rolling.classify(write.png.file = TRUE, classification.method = "svm", distance.measure="delta",
mfw = 1000, training.set.sampling = "normal.sampling", slice.size = 40, slice.overlap = 30,
delete.pronouns=FALSE, preserve.case=FALSE, corpus.lang="Latin",analyzed.features="c",ngram.size=3,culling=0)
#test sur les textes non-lemmatisés et les bigrammes de mots avec une liste éditée manuellement
# de sorte à n'y trouver que les mots-outils les plus fréquents
setwd("~/Documents/two_subcorpora1")
rolling.classify(write.png.file = TRUE, classification.method = "svm", distance.measure="delta", features="stopwords_bigrams.txt",
mfw = 300, training.set.sampling = "normal.sampling", slice.size = 40, slice.overlap = 30,
delete.pronouns=FALSE, preserve.case=FALSE, corpus.lang="Latin",analyzed.features="w",ngram.size=2,culling=0)
setwd("~/Documents/two_subcorpora")
rolling.classify(write.png.file = TRUE, classification.method = "svm", distance.measure="delta", features="wordlist_stopwords.txt",
mfw = 1000, training.set.sampling = "normal.sampling", slice.size = 40, slice.overlap = 30, culling=0,
delete.pronouns=FALSE, preserve.case=FALSE, corpus.lang="Latin")
rolling.classify(write.png.file = TRUE, classification.method = "svm", distance.measure="delta", features="stopwords.txt",
mfw = 63, training.set.sampling = "normal.sampling", slice.size = 40, slice.overlap = 30, culling=0,
delete.pronouns=FALSE, preserve.case=FALSE, corpus.lang="Latin")
#test sur les textes non-lemmatisés et sur les trigrammes de caractères
setwd("~/Documents/two_subcorpora1")
#test sur les textes non-lemmatisés et sur les trigrammes de caractères
setwd("~/Documents/two_subcorpora1")
rolling.classify(write.png.file = TRUE, classification.method = "svm", distance.measure="delta",
mfw = 1000, training.set.sampling = "normal.sampling", slice.size = 40, slice.overlap = 30,
delete.pronouns=FALSE, preserve.case=FALSE, corpus.lang="Latin",analyzed.features="c",ngram.size=3,culling=0)
#test sur les textes non-lemmatisés et les bigrammes de mots avec une liste éditée manuellement
# de sorte à n'y trouver que les mots-outils les plus fréquents
setwd("~/Documents/two_subcorpora1")
rolling.classify(write.png.file = TRUE, classification.method = "svm", distance.measure="delta", features="stopwords_bigrams.txt",
mfw = 300, training.set.sampling = "normal.sampling", slice.size = 40, slice.overlap = 30,
delete.pronouns=FALSE, preserve.case=FALSE, corpus.lang="Latin",analyzed.features="w",ngram.size=2,culling=0)
